# ğŸŒ Machine Translation Demo avec LLMs + Gradio

Application de dÃ©monstration pour comparer diffÃ©rents modÃ¨les de traduction automatique anglais-franÃ§ais avec Ã©valuation des performances.

## ğŸ“‹ Vue d'ensemble

Cette application compare **6 modÃ¨les de traduction** avec **5 tempÃ©ratures diffÃ©rentes** pour analyser les performances de traduction anglais-franÃ§ais :
- ğŸ¦™ **2 modÃ¨les Ollama** (locaux)
- ğŸ¤— **3 modÃ¨les HuggingFace** (fine-tunÃ©s sur OPUS Books)
- ğŸ“Š **Ã‰valuation automatique** avec mÃ©triques BLEU et ROUGE

## ğŸ¤– ModÃ¨les UtilisÃ©s

### **ModÃ¨les Ollama (Locaux)**
- **Llama 3.2:3b** - ModÃ¨le rapide et lÃ©ger pour traduction
- **Mistral 7b** - ModÃ¨le plus prÃ©cis pour textes complexes

### **ModÃ¨les HuggingFace (Fine-tunÃ©s)**
- **Marian-Finetuned** - BasÃ© sur `Helsinki-NLP/opus-mt-en-fr`
- **T5-Finetuned** - BasÃ© sur `google-t5/t5-small`
- **mBART-Finetuned** - BasÃ© sur `facebook/mbart-large-50-many-to-many-mmt`

## ğŸ“š Dataset

**OPUS Books** - Collection de livres libres de droits alignÃ©s en anglais-franÃ§ais
- **45,000 exemples** d'entraÃ®nement
- **5,000 exemples** de validation
- Textes littÃ©raires de qualitÃ© pour un entraÃ®nement robuste

## ğŸŒ¡ï¸ TempÃ©ratures TestÃ©es

- **0.0** - DÃ©terministe (traductions cohÃ©rentes)
- **0.2** - LÃ©gÃ¨rement crÃ©atif
- **0.5** - Ã‰quilibrÃ©
- **0.8** - Plus crÃ©atif
- **1.0** - TrÃ¨s crÃ©atif (moins prÃ©visible)

## ğŸ“Š MÃ©triques d'Ã‰valuation

### **BLEU Score**
Mesure la similaritÃ© entre traduction gÃ©nÃ©rÃ©e et rÃ©fÃ©rence
- **>40** : Excellent
- **20-40** : Bon
- **<20** : Faible

### **ROUGE Scores**
Mesure le recouvrement de n-grammes
- **ROUGE-1** : Mots uniques
- **ROUGE-2** : Bigrammes
- **ROUGE-L** : Plus longue sous-sÃ©quence commune

## ğŸš€ Installation et Usage

### **PrÃ©requis**
```bash
# Cloner le repository
git clone <votre-repo>
cd machine-translation-demo

# CrÃ©er l'environnement virtuel
python -m venv translator_env
source translator_env/bin/activate  # Linux/Mac
# ou translator_env\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### **Installer Ollama**
1. TÃ©lÃ©charger depuis [ollama.ai](https://ollama.ai)
2. Installer les modÃ¨les :
```bash
ollama pull llama3.2:3b
ollama pull mistral:7b
```

### **EntraÃ®ner les modÃ¨les HuggingFace**
```bash
python train_models_hugg.py
```

### **Lancer l'application**
```bash
python app.py
```

AccÃ©der Ã  l'interface : `http://localhost:7860`

## ğŸ“ Structure du Projet

```
machine-translation-demo/
â”œâ”€â”€ app.py                     # Application principale Gradio
â”œâ”€â”€ train_models_hugg.py       # Script d'entraÃ®nement des modÃ¨les
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â”œâ”€â”€ models/                    # ModÃ¨les fine-tunÃ©s
â”‚   â”œâ”€â”€ marian-finetuned-opus/
â”‚   â”œâ”€â”€ t5-finetuned-opus/
â”‚   â””â”€â”€ mbart-finetuned-opus/
â”œâ”€â”€ results/                   # RÃ©sultats d'Ã©valuation
â”‚   â”œâ”€â”€ evaluation_results.csv
â”‚   â”œâ”€â”€ bleu_scores.png
â”‚   â””â”€â”€ bleu_heatmap.png
â””â”€â”€ README.md
```

## ğŸ¯ FonctionnalitÃ©s

### **Interface Gradio avec 4 onglets :**

1. **ğŸ”¤ Traduction Interactive**
   - Test en temps rÃ©el des modÃ¨les
   - SÃ©lection du type de modÃ¨le et tempÃ©rature
   - Comparaison directe des rÃ©sultats

2. **ğŸ“Š Ã‰valuation ComplÃ¨te**
   - Ã‰valuation automatique sur 50 exemples du dataset
   - Calcul des scores BLEU et ROUGE
   - Barre de progression en temps rÃ©el

3. **ğŸ“ˆ RÃ©sultats DÃ©taillÃ©s**
   - Tableau comparatif des performances
   - Classement par mÃ©trique
   - Export des rÃ©sultats

4. **â„¹ï¸ Ã€ propos**
   - Documentation complÃ¨te
   - Explication des modÃ¨les et mÃ©triques

## ğŸ“Š RÃ©sultats de Performance

### **Classement par Score BLEU**
1. **Marian-Finetuned** : 49.02 â­
2. **T5-Finetuned** : 34.27
3. **mBART-Finetuned** : [En cours d'Ã©valuation]
4. **Mistral 7b** : 5-7
5. **Llama 3.2:3b** : 1-2

### **Observations**
- Les **modÃ¨les fine-tunÃ©s** surpassent largement les modÃ¨les Ollama
- **Marian** excelle en prÃ©cision (BLEU Ã©levÃ©)
- **T5** excelle en recouvrement (ROUGE Ã©levÃ©)
- La **tempÃ©rature** a peu d'impact sur les modÃ¨les fine-tunÃ©s

## ğŸ› ï¸ Technologies UtilisÃ©es

- **Interface** : Gradio 3.35+
- **ModÃ¨les** : Transformers 4.30+, Ollama
- **Ã‰valuation** : SacreBLEU, ROUGE Score
- **DonnÃ©es** : Datasets (HuggingFace)
- **Visualisation** : Matplotlib, Seaborn
- **Backend** : PyTorch

## ğŸ“ˆ AmÃ©liorations Futures

- [ ] Ajout de nouveaux modÃ¨les (NLLB, mT5)
- [ ] Support d'autres paires de langues
- [ ] Ã‰valuation sur datasets additionnels
- [ ] Interface web dÃ©ployÃ©e
- [ ] API REST pour intÃ©gration

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :
- Signaler des bugs
- Proposer de nouveaux modÃ¨les
- AmÃ©liorer l'interface
- Ajouter des mÃ©triques d'Ã©valuation

## ğŸ“ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ¯ Citation

Si vous utilisez ce projet dans vos recherches, veuillez citer :

```bibtex
@misc{machine-translation-demo,
  title={Machine Translation Demo with LLMs and Gradio},
  author={[Votre Nom]},
  year={2025},
  url={[URL de votre repository]}
}
```

---

**DÃ©veloppÃ© avec â¤ï¸ pour la communautÃ© NLP**